{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07404c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (4.56.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f597ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-docs in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U PyPDF2\n",
    "%pip install python-docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a449c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (5.1.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (1.7.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.56.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jneec\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers scikit-learn\n",
    "%pip install numpy\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "111e48b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jneec\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: imports & global setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import sqlite3\n",
    "import subprocess\n",
    "from io import BytesIO\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "print(torch.version.cuda)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7742b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_text(file_path) -> str:\n",
    "    if isinstance(file_path, (str, os.PathLike)):\n",
    "        reader = PdfReader(str(file_path))\n",
    "    else:\n",
    "        reader = PdfReader(file_path)\n",
    "    text = []\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text() or \"\"\n",
    "        text.append(page_text)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def read_text_file(file_path) -> str:\n",
    "    if isinstance(file_path, (str, os.PathLike)):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    else:\n",
    "        return file_path.read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79edd936",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_REGEX = r\"(Q\\d+):\\s*(.+?)\\s*A\\d+:\\s*(.+?)(?=\\nQ\\d+:|$)\"\n",
    "MARKS_IN_Q = r\"\\((\\d+)\\)\" \n",
    "\n",
    "def parse_exam(text: str, default_marks: int = 5) -> Tuple[Dict[str, dict], Dict[str, str]]:\n",
    "    \n",
    "    matches = re.findall(QA_REGEX, text, flags=re.DOTALL)\n",
    "    questions = {}\n",
    "    model_answers = {}\n",
    "    for q_id, q_text, a_text in matches:\n",
    "        m = re.search(MARKS_IN_Q, q_text)\n",
    "        marks = int(m.group(1)) if m else default_marks\n",
    "        clean_q_text = re.sub(MARKS_IN_Q, \"\", q_text).strip()\n",
    "        questions[q_id] = {\"text\": clean_q_text, \"marks\": marks}\n",
    "        model_answers[q_id] = a_text.strip()\n",
    "    if not questions:\n",
    "        raise ValueError(\"No Q/A pairs detected. Ensure format 'Q1: ... A1: ...'.\")\n",
    "    return questions, model_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b0c43ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trocr_processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "trocr_model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02bb53ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "POPPLER_PATH = r\"C:\\poppler-25.07.0\\Library\\bin\"\n",
    "def ocr_pdf_to_text(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a multi-page PDF to text using TrOCR.\n",
    "    \"\"\"\n",
    "    images = convert_from_path(pdf_path, poppler_path=POPPLER_PATH)\n",
    "    lines = []\n",
    "    for img in images:\n",
    "        pixel_values = trocr_processor(images=img, return_tensors=\"pt\").pixel_values\n",
    "        gen_ids = trocr_model.generate(pixel_values)\n",
    "        text = trocr_processor.batch_decode(gen_ids, skip_special_tokens=True)[0]\n",
    "        lines.append(text)\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c6b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e39b7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 200, overlap: int = 50):\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "    chunks, i = [], 0\n",
    "    while i < len(words):\n",
    "        chunks.append(\" \".join(words[i:i+chunk_size]))\n",
    "        i += max(chunk_size - overlap, 1)\n",
    "    return chunks\n",
    "\n",
    "def map_answers_to_questions(questions: Dict[str, dict], ocr_text: str,\n",
    "                             chunk_size=200, overlap=50, threshold=0.6) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Returns a dict: { \"Q1\": \"student text...\", \"Q2\": \"...\", \"Flagged\": \"low-confidence chunks\" }\n",
    "    \"\"\"\n",
    "    chunks = chunk_text(ocr_text, chunk_size=chunk_size, overlap=overlap)\n",
    "    q_keys = list(questions.keys())\n",
    "    q_texts = [q[\"text\"] for q in questions.values()]\n",
    "    q_emb = embedder.encode(q_texts)\n",
    "    mapped = {k: \"\" for k in q_keys}\n",
    "    flagged = []\n",
    "\n",
    "    for ch in chunks:\n",
    "        ch_emb = embedder.encode([ch])\n",
    "        sims = cosine_similarity(ch_emb, q_emb)[0]\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        best_sim = float(sims[best_idx])\n",
    "        if best_sim >= threshold:\n",
    "            mapped[q_keys[best_idx]] += ch + \" \"\n",
    "        else:\n",
    "            flagged.append(ch)\n",
    "\n",
    "    if flagged:\n",
    "        mapped[\"Flagged\"] = \" \".join(flagged)\n",
    "    return mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3cd72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_with_mistral(question_text: str,\n",
    "                       model_answer: str,\n",
    "                       student_answer: str,\n",
    "                       max_marks: int) -> dict:\n",
    "    \"\"\"\n",
    "    Calls `ollama run mistral` and expects JSON { \"Marks\": number, \"Feedback\": \"...\" }.\n",
    "    Uses proportional scoring based on max_marks and expected completeness.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "        You are an exam evaluator.\n",
    "\n",
    "        Question: {question_text}\n",
    "        Model Answer: {model_answer}\n",
    "        Student Answer: {student_answer}\n",
    "        Max Marks: {max_marks}\n",
    "\n",
    "        Scoring instructions:\n",
    "        - Grade strictly out of {max_marks}.\n",
    "        - Higher-mark questions require more coverage of key points for receiving full marks.\n",
    "        - Give partial marks proportionally based on correctness, completeness, specificity, and relevance.\n",
    "        - Reduce some marks if there are factual errors and irrelevant content.\n",
    "        - If the student's answer is unclear or unreadable, return Marks: 0 and Feedback: \"Answer Unclear!!! manual review is required\".\n",
    "        - Respond as strict JSON with keys \"Marks\" (number) and \"Feedback\" (string), nothing else.\n",
    "        JSON:\n",
    "        \"\"\".strip()\n",
    "\n",
    "    proc = subprocess.run(\n",
    "        [\"ollama\", \"run\", \"mistral\"],\n",
    "        input=prompt.encode(\"utf-8\"),\n",
    "        capture_output=True,\n",
    "        check=False\n",
    "    )\n",
    "    raw = proc.stdout.decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "    try:\n",
    "        start = raw.find(\"{\")\n",
    "        end = raw.rfind(\"}\")\n",
    "        payload = raw[start:end+1] if start != -1 and end != -1 else raw\n",
    "        data = json.loads(payload)\n",
    "        # normalize\n",
    "        marks = float(data.get(\"Marks\", 0))\n",
    "        feedback = str(data.get(\"Feedback\", \"\")).strip()\n",
    "        # clamp marks\n",
    "        if marks < 0: marks = 0\n",
    "        if marks > max_marks: marks = max_marks\n",
    "        return {\"Marks\": marks, \"Feedback\": feedback}\n",
    "    except Exception:\n",
    "        return {\"Marks\": 0.0, \"Feedback\": \"Unclear – manual review required (invalid LLM JSON).\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35bad072",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH = \"grades.db\"\n",
    "\n",
    "def init_db(db_path=DB_PATH):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS students (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        name TEXT,\n",
    "        uploaded_pdf TEXT\n",
    "    );\"\"\")\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS answers (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        student_id INTEGER,\n",
    "        question_number TEXT,\n",
    "        raw_ocr TEXT,\n",
    "        mapped_answer TEXT,\n",
    "        FOREIGN KEY (student_id) REFERENCES students(id)\n",
    "    );\"\"\")\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS grades (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        answer_id INTEGER,\n",
    "        marks REAL,\n",
    "        feedback TEXT,\n",
    "        FOREIGN KEY (answer_id) REFERENCES answers(id)\n",
    "    );\"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "init_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f80c5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_student(name: str, pdf_name: str, db_path=DB_PATH) -> int:\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT INTO students (name, uploaded_pdf) VALUES (?,?)\", (name, pdf_name))\n",
    "    sid = cur.lastrowid\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return sid\n",
    "\n",
    "def insert_answer(student_id: int, q_num: str, raw_ocr: str, mapped: str, db_path=DB_PATH) -> int:\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\n",
    "        \"INSERT INTO answers (student_id, question_number, raw_ocr, mapped_answer) VALUES (?,?,?,?)\",\n",
    "        (student_id, q_num, raw_ocr, mapped)\n",
    "    )\n",
    "    aid = cur.lastrowid\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return aid\n",
    "\n",
    "def insert_grade(answer_id: int, marks: float, feedback: str, db_path=DB_PATH):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\n",
    "        \"INSERT INTO grades (answer_id, marks, feedback) VALUES (?,?,?)\",\n",
    "        (answer_id, marks, feedback)\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd3bdbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_exam(exam_path: str) -> Tuple[Dict[str, dict], Dict[str, str]]:\n",
    "    ext = os.path.splitext(exam_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        text = read_pdf_text(exam_path)\n",
    "    elif ext == \".txt\":\n",
    "        text = read_text_file(exam_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported exam file type; use .pdf or .txt\")\n",
    "    return parse_exam(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e12a589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_batch(exam_path: str, student_pdf_paths: list) -> dict:\n",
    "    questions, model_answers = load_exam(exam_path)\n",
    "    results = {}\n",
    "    for pdf_path in student_pdf_paths:\n",
    "        # 1) OCR\n",
    "        raw = ocr_pdf_to_text(pdf_path)\n",
    "\n",
    "        # 2) Map to questions\n",
    "        mapped = map_answers_to_questions(questions, raw, chunk_size=220, overlap=80, threshold=0.6)\n",
    "\n",
    "        # 3) Store student\n",
    "        student_name = os.path.basename(pdf_path)\n",
    "        sid = insert_student(student_name, student_name)\n",
    "\n",
    "        # 4) Grade per question\n",
    "        student_result = {}\n",
    "        for q_id, q_meta in questions.items():\n",
    "            student_ans = mapped.get(q_id, \"\").strip()\n",
    "            aid = insert_answer(sid, q_id, raw, student_ans)\n",
    "            graded = grade_with_mistral(q_meta[\"text\"], model_answers[q_id], student_ans, q_meta[\"marks\"])\n",
    "            insert_grade(aid, graded[\"Marks\"], graded[\"Feedback\"])\n",
    "            student_result[q_id] = graded\n",
    "\n",
    "        # optionally capture flagged chunks\n",
    "        if \"Flagged\" in mapped and mapped[\"Flagged\"].strip():\n",
    "            student_result[\"Flagged\"] = mapped[\"Flagged\"].strip()\n",
    "\n",
    "        results[student_name] = student_result\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b0c160f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"s1.pdf\": {\n",
      "    \"Q1\": {\n",
      "      \"Marks\": 4.5,\n",
      "      \"Feedback\": \"The student has correctly differentiated between e-Government and e-Governance, explaining their definitions, focus, communication, scope, and objectives with examples. However, the stages of e-Governance are explained but not described in detail as per Gartner\\u2019s Four-Stage Model.\"\n",
      "    },\n",
      "    \"Q2\": {\n",
      "      \"Marks\": 4.0,\n",
      "      \"Feedback\": \"The answer provides a good definition of e-Government and outlines some key components when viewed as an Information System. However, it could be more specific about software applications, data and databases, processes and workflows, people and stakeholders, security and governance aspects.\"\n",
      "    },\n",
      "    \"Flagged\": \"to the next time a b What you should be too much to be a good sense of the\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "exam_path = \"./data/exam_files/qna.txt\"  # or .txt\n",
    "student_pdf_paths = [\n",
    "    \"data/student_pdfs/s1.pdf\"\n",
    "]\n",
    "results = grade_batch(exam_path, student_pdf_paths)\n",
    "print(json.dumps(results, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
